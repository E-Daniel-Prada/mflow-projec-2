```markdown
# ğŸš€ MLflow API + Gradio - Inferencia y GestiÃ³n de Modelos

Este proyecto implementa una soluciÃ³n completa de despliegue de modelos con **MLflow**, integrando:

- ğŸ”§ Una **API FastAPI** para inferencia y gestiÃ³n de modelos
- ğŸ’¡ Una **interfaz Gradio** para probar predicciones fÃ¡cilmente
- ğŸ“¦ ConexiÃ³n con **MinIO** para carga directa de modelos
- ğŸ³ Despliegue con Docker + Docker Compose

---

## ğŸ“‚ Estructura del Proyecto

```
mlflow-api/
â”‚â”€â”€ Dockerfile                   # Imagen base para la API + Gradio
â”‚â”€â”€ docker-compose.yml           # OrquestaciÃ³n de servicios
â”‚â”€â”€ requirements.txt             # LibrerÃ­as necesarias
â”‚â”€â”€ .env                         # Variables de entorno
â”‚â”€â”€ start.sh                     # Script de arranque (API + Gradio)
â”‚
â”‚â”€â”€ app/
â”‚   â”œâ”€â”€ main.py                  # FastAPI con endpoints REST
â”‚   â”œâ”€â”€ interface.py             # Interfaz visual Gradio
â”‚   â”œâ”€â”€ inference.py             # LÃ³gica para predicciÃ³n y carga de modelos
â”‚   â”œâ”€â”€ register.py              # Script auxiliar para registrar modelos
â”‚   â”œâ”€â”€ __init__.py              # Inicializador del mÃ³dulo
```

---

## ğŸ“Œ Funcionalidades

âœ… Carga de modelos desde **MLflow Registry** o directamente desde **MinIO**  
âœ… Predicciones vÃ­a API o Gradio  
âœ… Carga dinÃ¡mica de modelos mediante `run_id`  
âœ… Consulta de experimentos y ejecuciones (runs)  
âœ… VisualizaciÃ³n y test interactivo con Gradio  
âœ… Despliegue rÃ¡pido con Docker

---

## ğŸ”§ Requisitos

- Docker y Docker Compose
- Acceso a un servidor MLflow y MinIO (puede ser externo)
- Python 3.10+ (solo si ejecutas localmente sin Docker)

---

## âš™ï¸ ConfiguraciÃ³n

Edita el archivo `.env` con los valores adecuados:

```env
MLFLOW_TRACKING_URI=http://10.43.101.184:5000
MLFLOW_S3_ENDPOINT_URL=http://10.43.101.184:9000
AWS_ACCESS_KEY_ID=admin
AWS_SECRET_ACCESS_KEY=supersecret
```

---

## ğŸš€ Despliegue

```bash
sudo docker-compose up -d --build
```

---

## ğŸŒ Acceso a la App

| Servicio       | URL                     |
|----------------|--------------------------|
| API FastAPI    | http://localhost:8000/docs |
| Interfaz Gradio| http://localhost:8503     |

---

## ğŸ” Endpoints REST disponibles

### âœ… Estado de la API
```
GET /
```

### âœ… PredicciÃ³n
```
POST /predict/
{
  "features": [1000, 5.5]
}
```

### âœ… Listar modelos registrados
```
GET /models
```

### âœ… Obtener detalles de experimento
```
GET /experiment/{experiment_name}
```

### âœ… Obtener detalles de una ejecuciÃ³n
```
GET /run/{run_id}
```

### âœ… Cargar modelo desde MinIO
```
GET /load-model-from-minio/{run_id}?artifact_path=model&experiment_id=5
```

---

## ğŸ§ª Interfaz Gradio

La interfaz web permite:

- Cargar un modelo desde MinIO
- Especificar `run_id` y `artifact_path`
- Ingresar valores para predicciÃ³n
- Ver detalles del modelo y mÃ©tricas
- Explorar experimentos disponibles

Accede a:  
```
http://localhost:8503
```

---

## ğŸ” Internamente

### Carga desde MinIO

```python
model = load_model_from_minio(run_id, artifact_subpath="model", experiment_id="5")
```

### Realizar predicciÃ³n

```python
def make_prediction(model, input_data):
    if model is None:
        return {"error": "No hay modelos disponibles"}
    prediction = model.predict(input_data)
    return {"prediction": prediction.tolist()}
```

---

## ğŸ“Œ Ejemplo visual en MinIO

Ruta tÃ­pica de un modelo en MinIO:
```
artifacts/{experiment_id}/{run_id}/artifacts/model/MLmodel
```
![alt text](image.png)
---

## ğŸ¯ ConclusiÃ³n

Este proyecto facilita el consumo de modelos en entornos de producciÃ³n, integrando tecnologÃ­as modernas como:

- ğŸ§ª MLflow para gestiÃ³n de modelos
- â˜ï¸ MinIO para almacenamiento de artefactos
- ğŸš€ FastAPI para una API ligera y extensible
- ğŸ’¡ Gradio para visualizaciÃ³n interactiva